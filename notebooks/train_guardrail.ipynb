{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ModernBERT Hallucination Grader Training\n",
                "**Optimization: ModernBERT Hallucination Detector**\n",
                "\n",
                "This notebook fine-tunes `answerdotai/ModernBERT-base` for binary hallucination detection.\n",
                "\n",
                "## Target Metrics\n",
                "- Latency: <15ms on T4\n",
                "- Context: 8k tokens\n",
                "- Task: Binary classification (0=hallucinated, 1=faithful)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Install dependencies\n",
                "!pip install -q transformers torch accelerate datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "from transformers import AutoTokenizer, AutoModel\n",
                "import json\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Check GPU\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Load training data generated for the grader\n",
                "def load_jsonl(path):\n",
                "    data = []\n",
                "    with open(path, 'r') as f:\n",
                "        for line in f:\n",
                "            data.append(json.loads(line))\n",
                "    return data\n",
                "\n",
                "# Upload your hallucination_dataset.jsonl to Colab first\n",
                "dataset = load_jsonl('hallucination_dataset.jsonl')\n",
                "print(f'Loaded {len(dataset)} samples')\n",
                "\n",
                "# Train/Val split (80/20)\n",
                "split_idx = int(len(dataset) * 0.8)\n",
                "train_data = dataset[:split_idx]\n",
                "val_data = dataset[split_idx:]\n",
                "print(f'Train: {len(train_data)}, Val: {len(val_data)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Dataset & Model Definition"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "class HallucinationDataset(Dataset):\n",
                "    def __init__(self, data, tokenizer, max_length=512):\n",
                "        self.data = data\n",
                "        self.tokenizer = tokenizer\n",
                "        self.max_length = max_length\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.data)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        item = self.data[idx]\n",
                "        encoding = self.tokenizer(\n",
                "            item['text'],\n",
                "            truncation=True,\n",
                "            max_length=self.max_length,\n",
                "            padding='max_length',\n",
                "            return_tensors='pt'\n",
                "        )\n",
                "        return {\n",
                "            'input_ids': encoding['input_ids'].squeeze(),\n",
                "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
                "            'label': torch.tensor(item['label'], dtype=torch.long)\n",
                "        }\n",
                "\n",
                "\n",
                "class ModernBERTClassifier(nn.Module):\n",
                "    \"\"\"ModernBERT with classification head for hallucination detection.\"\"\"\n",
                "    \n",
                "    def __init__(self, model_name='answerdotai/ModernBERT-base', num_labels=2):\n",
                "        super().__init__()\n",
                "        self.bert = AutoModel.from_pretrained(model_name)\n",
                "        self.dropout = nn.Dropout(0.1)\n",
                "        self.classifier = nn.Linear(768, num_labels)  # ModernBERT hidden size = 768\n",
                "    \n",
                "    def forward(self, input_ids, attention_mask):\n",
                "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
                "        # Use [CLS] token representation\n",
                "        pooled = outputs.last_hidden_state[:, 0, :]\n",
                "        pooled = self.dropout(pooled)\n",
                "        logits = self.classifier(pooled)\n",
                "        return logits"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Loop (Custom PyTorch - NOT Trainer API)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Initialize\n",
                "MODEL_NAME = 'answerdotai/ModernBERT-base'\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "model = ModernBERTClassifier(MODEL_NAME).to(device)\n",
                "\n",
                "# Dataloaders\n",
                "train_dataset = HallucinationDataset(train_data, tokenizer)\n",
                "val_dataset = HallucinationDataset(val_data, tokenizer)\n",
                "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
                "val_loader = DataLoader(val_dataset, batch_size=16)\n",
                "\n",
                "# Optimizer & Loss\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "\n",
                "print(f'Model parameters: {sum(p.numel() for p in model.parameters()):,}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def train_epoch(model, loader, optimizer, criterion):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    for batch in tqdm(loader, desc='Training'):\n",
                "        input_ids = batch['input_ids'].to(device)\n",
                "        attention_mask = batch['attention_mask'].to(device)\n",
                "        labels = batch['label'].to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        logits = model(input_ids, attention_mask)\n",
                "        loss = criterion(logits, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        preds = torch.argmax(logits, dim=1)\n",
                "        correct += (preds == labels).sum().item()\n",
                "        total += labels.size(0)\n",
                "    \n",
                "    return total_loss / len(loader), correct / total\n",
                "\n",
                "\n",
                "def evaluate(model, loader, criterion):\n",
                "    model.eval()\n",
                "    total_loss = 0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for batch in tqdm(loader, desc='Evaluating'):\n",
                "            input_ids = batch['input_ids'].to(device)\n",
                "            attention_mask = batch['attention_mask'].to(device)\n",
                "            labels = batch['label'].to(device)\n",
                "            \n",
                "            logits = model(input_ids, attention_mask)\n",
                "            loss = criterion(logits, labels)\n",
                "            \n",
                "            total_loss += loss.item()\n",
                "            preds = torch.argmax(logits, dim=1)\n",
                "            correct += (preds == labels).sum().item()\n",
                "            total += labels.size(0)\n",
                "    \n",
                "    return total_loss / len(loader), correct / total"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Training Loop - 5 Epochs\n",
                "EPOCHS = 5\n",
                "best_val_acc = 0\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    print(f'\\n=== Epoch {epoch+1}/{EPOCHS} ===')\n",
                "    \n",
                "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)\n",
                "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
                "    \n",
                "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
                "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
                "    \n",
                "    # Save best model\n",
                "    if val_acc > best_val_acc:\n",
                "        best_val_acc = val_acc\n",
                "        torch.save(model.state_dict(), 'guardrail_v1.pt')\n",
                "        print(f'\u00e2\u0153\u201c Saved best model (val_acc: {val_acc:.4f})')\n",
                "\n",
                "print(f'\\n\u00e2\u0153\u201c Training complete! Best Val Accuracy: {best_val_acc:.4f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Inference Speed Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import time\n",
                "\n",
                "# Load best model\n",
                "model.load_state_dict(torch.load('guardrail_v1.pt'))\n",
                "model.eval()\n",
                "\n",
                "# Test inference speed\n",
                "test_text = 'Context: The company reported 15% growth. Answer: Revenue increased by 15%.'\n",
                "inputs = tokenizer(test_text, return_tensors='pt', truncation=True, max_length=512)\n",
                "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
                "\n",
                "# Warmup\n",
                "for _ in range(10):\n",
                "    with torch.no_grad():\n",
                "        _ = model(inputs['input_ids'], inputs['attention_mask'])\n",
                "\n",
                "# Benchmark\n",
                "times = []\n",
                "for _ in range(100):\n",
                "    start = time.perf_counter()\n",
                "    with torch.no_grad():\n",
                "        logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
                "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
                "    times.append((time.perf_counter() - start) * 1000)\n",
                "\n",
                "print(f'Average inference time: {sum(times)/len(times):.2f}ms')\n",
                "print(f'Target: <15ms \u00e2\u0153\u201c' if sum(times)/len(times) < 15 else 'Target: <15ms \u00e2\u0153\u2014')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Export Model\n",
                "Download `guardrail_v1.pt` and place in `models/` folder."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "from google.colab import files\n",
                "files.download('guardrail_v1.pt')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}